{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yTaUpjBfH4NK"
      },
      "outputs": [],
      "source": [
        "# !pip -q install earthengine-api pandas numpy scikit-learn\n",
        "# Authentication from terminal is required before tunning this notebook\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRu3Mwa_IKnX"
      },
      "outputs": [],
      "source": [
        "import ee\n",
        "ee.Authenticate()\n",
        "ee.Initialize(project='ee-anapaolagomes')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# MODIS land cover (IGBP classes in band \"LC_Type1\")\n",
        "MODIS_LC = ee.ImageCollection(\"MODIS/061/MCD12Q1\")\n",
        "\n",
        "\n",
        "def modis_lc_for_year(year: int, region: ee.Geometry) -> ee.Image:\n",
        "    return (\n",
        "        MODIS_LC.filter(ee.Filter.calendarRange(year, year, \"year\"))\n",
        "        .first()\n",
        "        .clip(region)\n",
        "    )\n",
        "\n",
        "\n",
        "def is_forest_igbp(lc_type1: ee.Image) -> ee.Image:\n",
        "    \"\"\"\n",
        "    Forest mask for IGBP land cover classes (strict forest):\n",
        "      1 Evergreen Needleleaf Forests\n",
        "      2 Evergreen Broadleaf Forests\n",
        "      3 Deciduous Needleleaf Forests\n",
        "      4 Deciduous Broadleaf Forests\n",
        "      5 Mixed Forests\n",
        "\n",
        "    Input: lc_type1 is the MODIS \"LC_Type1\" band image (integer classes).\n",
        "    Output: boolean ee.Image mask (True where forest).\n",
        "    \"\"\"\n",
        "    return (\n",
        "        lc_type1.eq(1)\n",
        "        .Or(lc_type1.eq(2))\n",
        "        .Or(lc_type1.eq(3))\n",
        "        .Or(lc_type1.eq(4))\n",
        "        .Or(lc_type1.eq(5))\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "paPlQOHlUnIV"
      },
      "outputs": [],
      "source": [
        "ROADS_BR = ee.FeatureCollection(\"projects/sat-io/open-datasets/GRIP4/Central-South-America\")\n",
        "\n",
        "def frontier_features_for_year(t_year: int, region: ee.Geometry, scale: int) -> ee.Image:\n",
        "    lc_t = modis_lc_for_year(t_year, region).select(\"LC_Type1\")\n",
        "    forest = is_forest_igbp(lc_t).rename(\"forest\").toByte()\n",
        "    nonforest = forest.Not().rename(\"nonforest\").toByte()\n",
        "\n",
        "    pix_m = ee.Number(lc_t.projection().nominalScale())\n",
        "\n",
        "    # Distance to nearest non-forest (within forest pixels)\n",
        "    dist_nf = (\n",
        "        nonforest.selfMask()\n",
        "        .fastDistanceTransform(256)\n",
        "        .sqrt()\n",
        "        .multiply(pix_m)\n",
        "        .rename(\"dist_to_nonforest_m\")\n",
        "        .updateMask(forest)\n",
        "    )\n",
        "\n",
        "    # Distance to roads (meters); limit search radius for speed (e.g. 100km)\n",
        "    dist_rd = (\n",
        "        ROADS_BR.filterBounds(region)\n",
        "        .distance(searchRadius=100000)\n",
        "        .rename(\"dist_to_road_m\")\n",
        "    )\n",
        "\n",
        "    return dist_nf.addBands(dist_rd)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d54wMJp1H90A",
        "outputId": "07b12b31-4a3c-4959-a109-e01e9c5787c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AEF bands: ['A00', 'A01', 'A02', 'A03', 'A04', 'A05', 'A06', 'A07'] ... total: 64\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ===================== CONFIG =====================\n",
        "ROI = ee.Geometry.Rectangle([-63.5, -10.5, -61.5, -8.5]) \n",
        "SCALE = 500\n",
        "SEED = 42\n",
        "\n",
        "# Train on these years t (label is loss in (t, t+1])\n",
        "TRAIN_YEARS = [2018, 2019, 2020]\n",
        "\n",
        "# Stratified sample sizes per year (balanced)\n",
        "N_POS_PER_YEAR = 5000\n",
        "N_NEG_PER_YEAR = 5000\n",
        "\n",
        "# Unbiased eval (forest-only) year t\n",
        "UNBIASED_EVAL_YEAR = 2022\n",
        "N_UNBIASED_FOREST_PIXELS = 30000 \n",
        "\n",
        "# 2 options\n",
        "#   False = forest(t)=1 and forest(t+1)=0\n",
        "#   True  = \"stable\" label: pos requires forest(t-1)&forest(t)&nonforest(t+1); neg requires forest(t-1)&forest(t)&forest(t+1)\n",
        "USE_STABLE_LABEL = False\n",
        "\n",
        "\n",
        "AEF_IC = ee.ImageCollection(\"GOOGLE/SATELLITE_EMBEDDING/V1/ANNUAL\")\n",
        "MODIS_LC = ee.ImageCollection(\"MODIS/061/MCD12Q1\")  # yearly landcover\n",
        "\n",
        "def aef_for_year(year: int, region: ee.Geometry) -> ee.Image:\n",
        "    start = ee.Date.fromYMD(year, 1, 1)\n",
        "    end = start.advance(1, \"year\")\n",
        "    return AEF_IC.filterDate(start, end).filterBounds(region).mosaic().clip(region)\n",
        "\n",
        "\n",
        "def label_basic_loss(t_year: int, region: ee.Geometry) -> ee.Image:\n",
        "    lc_t  = modis_lc_for_year(t_year, region).select(\"LC_Type1\")\n",
        "    lc_t1 = modis_lc_for_year(t_year + 1, region).select(\"LC_Type1\")\n",
        "    forest_t  = is_forest_igbp(lc_t)\n",
        "    forest_t1 = is_forest_igbp(lc_t1)\n",
        "\n",
        "    y = forest_t.And(forest_t1.Not()).rename(\"label\").toByte()\n",
        "    valid = lc_t.mask().And(lc_t1.mask())\n",
        "    return y.updateMask(valid)\n",
        "\n",
        "def label_stable_loss(t_year: int, region: ee.Geometry) -> ee.Image:\n",
        "    # pos = forest(t-1)=1 & forest(t)=1 & forest(t+1)=0\n",
        "    # neg = forest(t-1)=1 & forest(t)=1 & forest(t+1)=1\n",
        "    lc_tm1 = modis_lc_for_year(t_year - 1, region).select(\"LC_Type1\")\n",
        "    lc_t   = modis_lc_for_year(t_year, region).select(\"LC_Type1\")\n",
        "    lc_tp1 = modis_lc_for_year(t_year + 1, region).select(\"LC_Type1\")\n",
        "\n",
        "    f_tm1 = is_forest_igbp(lc_tm1)\n",
        "    f_t   = is_forest_igbp(lc_t)\n",
        "    f_tp1 = is_forest_igbp(lc_tp1)\n",
        "\n",
        "    pos = f_tm1.And(f_t).And(f_tp1.Not())\n",
        "    neg = f_tm1.And(f_t).And(f_tp1)\n",
        "\n",
        "    y = pos.rename(\"label\").toByte()  # 1 where pos\n",
        "    y = y.where(neg, 0)               # 0 where neg\n",
        "\n",
        "    keep = pos.Or(neg)\n",
        "    valid = lc_tm1.mask().And(lc_t.mask()).And(lc_tp1.mask())\n",
        "    return y.updateMask(keep).updateMask(valid)\n",
        "\n",
        "def label_for_year(t_year: int, region: ee.Geometry) -> ee.Image:\n",
        "    return label_stable_loss(t_year, region) if USE_STABLE_LABEL else label_basic_loss(t_year, region)\n",
        "\n",
        "def sampling_image_for_year(t_year: int, region: ee.Geometry) -> ee.Image:\n",
        "    X = aef_for_year(t_year, region)                 # A00..A63\n",
        "    F = frontier_features_for_year(t_year, region, SCALE)  # dist bands\n",
        "    y = label_for_year(t_year, region)               # label\n",
        "    return X.addBands(F).addBands(y)\n",
        "\n",
        "def stratified_samples_for_year(t_year: int, region: ee.Geometry,\n",
        "                                n_neg: int, n_pos: int,\n",
        "                                scale: int, seed: int) -> ee.FeatureCollection:\n",
        "    img = sampling_image_for_year(t_year, region)\n",
        "\n",
        "    fc = img.stratifiedSample(\n",
        "        numPoints=1,                       # required even when using classPoints\n",
        "        classBand=\"label\",\n",
        "        classValues=[0, 1],\n",
        "        classPoints=[n_neg, n_pos],\n",
        "        region=region,\n",
        "        scale=scale,\n",
        "        seed=seed + t_year,\n",
        "        dropNulls=True,\n",
        "        tileScale=4,\n",
        "        geometries=True                    # keep geometry => exports .geo\n",
        "    )\n",
        "    return fc.map(lambda f: f.set({\"tYear\": t_year}))\n",
        "\n",
        "def forest_mask_for_year(t_year: int, region: ee.Geometry) -> ee.Image:\n",
        "    lc_t = modis_lc_for_year(t_year, region).select(\"LC_Type1\")\n",
        "    forest = is_forest_igbp(lc_t).rename(\"forest\").toByte()\n",
        "    return forest.updateMask(lc_t.mask())\n",
        "\n",
        "def unbiased_forest_samples(t_year: int, region: ee.Geometry,\n",
        "                            n_pixels: int, scale: int, seed: int) -> ee.FeatureCollection:\n",
        "    img = sampling_image_for_year(t_year, region)  # AEF + label\n",
        "    forest = forest_mask_for_year(t_year, region)\n",
        "\n",
        "    img_forest = img.updateMask(forest)\n",
        "\n",
        "    fc = img_forest.sample(\n",
        "        region=region,\n",
        "        scale=scale,\n",
        "        numPixels=n_pixels,\n",
        "        seed=seed + 999,\n",
        "        tileScale=4,\n",
        "        geometries=True\n",
        "    ).map(lambda f: f.set({\"tYear\": t_year, \"unbiased\": 1}))\n",
        "\n",
        "    return fc\n",
        "\n",
        "def export_fc_to_drive(fc: ee.FeatureCollection, description: str, filename_prefix: str, selectors: list[str]):\n",
        "    task = ee.batch.Export.table.toDrive(\n",
        "        collection=fc.select(selectors),\n",
        "        description=description,\n",
        "        fileNamePrefix=filename_prefix,\n",
        "        fileFormat=\"CSV\"\n",
        "    )\n",
        "    task.start()\n",
        "    return task\n",
        "\n",
        "# ===================== BUILD + EXPORT =====================\n",
        "\n",
        "# Determine embedding band names once (ensures correct ordering)\n",
        "bands = aef_for_year(TRAIN_YEARS[0], ROI).bandNames()\n",
        "band_list = bands.getInfo()\n",
        "print(\"AEF bands:\", band_list[:8], \"... total:\", len(band_list))\n",
        "frontier_band_list = [\"dist_to_nonforest_m\", \"dist_to_road_m\"]\n",
        "\n",
        "train_selectors = band_list + frontier_band_list + [\"label\", \"tYear\"]\n",
        "unbiased_selectors = band_list + frontier_band_list + [\"label\", \"tYear\", \"unbiased\"]\n",
        "\n",
        "selectors = band_list + [\"label\", \"tYear\", \"unbiased\"]  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "idpoEFvfRy_o"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Started TRAIN export: aef_train_balanced_2018_2020_v5\n",
            "Started UNBIASED export: aef_unbiased_forest_eval_2022_v5\n",
            "\n",
            "Now go to the Earth Engine Code Editor -> Tasks tab to monitor, or check Google Drive after completion.\n"
          ]
        }
      ],
      "source": [
        "# ---- 1) Training samples (balanced stratified, merged across years) ----\n",
        "train_fc = ee.FeatureCollection([\n",
        "    stratified_samples_for_year(y, ROI, N_NEG_PER_YEAR, N_POS_PER_YEAR, SCALE, SEED)\n",
        "    for y in TRAIN_YEARS\n",
        "]).flatten()\n",
        "\n",
        "train_desc = f\"aef_train_balanced_{min(TRAIN_YEARS)}_{max(TRAIN_YEARS)}_v5\" + (\"_stablelabel\" if USE_STABLE_LABEL else \"\")\n",
        "train_task = export_fc_to_drive(\n",
        "    train_fc,\n",
        "    description=train_desc,\n",
        "    filename_prefix=train_desc,\n",
        "    selectors=train_selectors\n",
        ")\n",
        "print(\"Started TRAIN export:\", train_desc)\n",
        "\n",
        "# ---- 2) Unbiased eval samples (forest-only, random sample) ----\n",
        "unb_fc = unbiased_forest_samples(UNBIASED_EVAL_YEAR, ROI, N_UNBIASED_FOREST_PIXELS, SCALE, SEED)\n",
        "\n",
        "unb_desc = f\"aef_unbiased_forest_eval_{UNBIASED_EVAL_YEAR}_v5\" + (\"_stablelabel\" if USE_STABLE_LABEL else \"\")\n",
        "unb_task = export_fc_to_drive(\n",
        "    unb_fc,\n",
        "    description=unb_desc,\n",
        "    filename_prefix=unb_desc,\n",
        "    selectors=unbiased_selectors\n",
        ")\n",
        "print(\"Started UNBIASED export:\", unb_desc)\n",
        "\n",
        "print(\"\\nNow go to the Earth Engine Code Editor -> Tasks tab to monitor, or check Google Drive after completion.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wOxg9LNJa1b",
        "outputId": "954daeeb-e03b-4d61-8b41-851e90000882"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train split: [2018, 2019, 2020] n= 23096 pos rate= 0.35053688950467615\n",
            "Test year: 2020 n= 8057 pos rate= 0.37942162095072607\n",
            "Unbiased year: 2022 n= 14026 pos rate= 0.029516612006274062\n",
            "\n",
            "=== Logit: time-split test (balanced) ===\n",
            "n: 8057 pos rate: 0.37942162095072607\n",
            "ROC-AUC: 0.9576175335296042\n",
            "PR-AUC: 0.918954808358895\n",
            "LogLoss: 0.25535842366918876\n",
            "Brier: 0.0771487339442452\n",
            "\n",
            "--- Logit time-split (thr=0.5) @ thr=0.5000 ---\n",
            "Confusion:\n",
            " [[4560  440]\n",
            " [ 414 2643]]\n",
            "Accuracy: 0.894005212858384\n",
            "Balanced accuracy: 0.8882865554465162\n",
            "Precision: 0.8572818683100876\n",
            "Recall: 0.8645731108930323\n",
            "Top  1%: thr=0.9977  capture=0.026  precision=1.000\n",
            "Top  2%: thr=0.9957  capture=0.052  precision=0.975\n",
            "Top  5%: thr=0.9890  capture=0.128  precision=0.968\n",
            "Top 10%: thr=0.9732  capture=0.254  precision=0.965\n",
            "\n",
            "=== Logit: unbiased forest-only ===\n",
            "n: 14026 pos rate: 0.029516612006274062\n",
            "ROC-AUC: 0.9449728039056188\n",
            "PR-AUC: 0.37074392976545834\n",
            "LogLoss: 0.268887388241014\n",
            "Brier: 0.08268332289189215\n",
            "\n",
            "--- Logit unbiased (thr=0.5) @ thr=0.5000 ---\n",
            "Confusion:\n",
            " [[12094  1518]\n",
            " [   61   353]]\n",
            "Accuracy: 0.8874233566234137\n",
            "Balanced accuracy: 0.8705688785541601\n",
            "Precision: 0.18866916087653662\n",
            "Recall: 0.8526570048309179\n",
            "Top  1%: thr=0.9788  capture=0.179  precision=0.525\n",
            "Top  2%: thr=0.9568  capture=0.307  precision=0.452\n",
            "Top  5%: thr=0.8767  capture=0.543  precision=0.321\n",
            "Top 10%: thr=0.6605  capture=0.758  precision=0.224\n",
            "\n",
            "=== Logit: unbiased (calibrated) ===\n",
            "n: 14026 pos rate: 0.029516612006274062\n",
            "ROC-AUC: 0.9449670367578478\n",
            "PR-AUC: 0.3707439297654583\n",
            "LogLoss: 0.08036444091275752\n",
            "Brier: 0.022443163617142995\n",
            "\n",
            "--- Logit calibrated unbiased (thr=0.5) @ thr=0.5000 ---\n",
            "Confusion:\n",
            " [[13567    45]\n",
            " [  353    61]]\n",
            "Accuracy: 0.9716241266219877\n",
            "Balanced accuracy: 0.5720185443080203\n",
            "Precision: 0.5754716981132075\n",
            "Recall: 0.1473429951690821\n",
            "Top  1%: thr=0.4466  capture=0.179  precision=0.525\n",
            "Top  2%: thr=0.3195  capture=0.307  precision=0.452\n",
            "Top  5%: thr=0.1690  capture=0.543  precision=0.321\n",
            "Top 10%: thr=0.0725  capture=0.758  precision=0.224\n",
            "\n",
            "=== MENAGERIE PARAMS (LOGIT, RAW SPACE) ===\n",
            "b: -4.221936447\n",
            "w: 12.24030952,10.98641265,3.253119464,-1.466683009,-7.197782255,3.912564667,0.5165286049,11.00165292,-22.42232696,-21.49251167,19.54616472,-3.96374875,16.17681181 ... (len= 826 )\n",
            "\n",
            "=== SVM (calibrated): unbiased forest-only ===\n",
            "n: 14026 pos rate: 0.029516612006274062\n",
            "ROC-AUC: 0.9444994541616448\n",
            "PR-AUC: 0.3636186969914364\n",
            "LogLoss: 0.08044608457400475\n",
            "Brier: 0.022501229146427788\n",
            "\n",
            "--- SVM calibrated unbiased (thr=0.5) @ thr=0.5000 ---\n",
            "Confusion:\n",
            " [[13570    42]\n",
            " [  355    59]]\n",
            "Accuracy: 0.9716954227862541\n",
            "Balanced accuracy: 0.5697132822559237\n",
            "Precision: 0.5841584158415841\n",
            "Recall: 0.14251207729468598\n",
            "Top  1%: thr=0.4326  capture=0.179  precision=0.525\n",
            "Top  2%: thr=0.3250  capture=0.292  precision=0.431\n",
            "Top  5%: thr=0.1796  capture=0.546  precision=0.322\n",
            "Top 10%: thr=0.0731  capture=0.761  precision=0.225\n",
            "\n",
            "Done.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.frozen import FrozenEstimator\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score, average_precision_score, log_loss, brier_score_loss,\n",
        "    confusion_matrix, accuracy_score, balanced_accuracy_score\n",
        ")\n",
        "\n",
        "\n",
        "# ===================== PATHS =====================\n",
        "TRAIN_CSV = \"/app/data/aef_train_balanced_2018_2020_v5.csv\"   \n",
        "UNBIASED_CSV =  \"/app/data/aef_unbiased_forest_eval_2022_v5.csv\"   \n",
        "# ===================== SETTINGS =====================\n",
        "TRAIN_YEARS = [2018, 2019, 2020]   # time split\n",
        "TEST_YEAR   = 2020\n",
        "\n",
        "# Alert budgets (map-style evaluation)\n",
        "TOP_K_LIST = [1, 2, 5, 10]  # %\n",
        "\n",
        "# ===================== LOAD =====================\n",
        "df = pd.read_csv(TRAIN_CSV)\n",
        "df_u = pd.read_csv(UNBIASED_CSV)\n",
        "\n",
        "feature_cols = [f\"A{i:02d}\" for i in range(64)] + [\"dist_to_nonforest_m\", \"dist_to_road_m\"]\n",
        "\n",
        "assert all(c in df.columns for c in feature_cols), \"Train CSV missing A00..A63\"\n",
        "assert all(c in df_u.columns for c in feature_cols), \"Unbiased CSV missing A00..A63\"\n",
        "\n",
        "# ===================== METRICS =====================\n",
        "def eval_probs(y, p, name=\"\"):\n",
        "    p = np.clip(p, 1e-6, 1 - 1e-6)\n",
        "    print(f\"\\n=== {name} ===\")\n",
        "    print(\"n:\", len(y), \"pos rate:\", float(y.mean()))\n",
        "    print(\"ROC-AUC:\", roc_auc_score(y, p))\n",
        "    print(\"PR-AUC:\", average_precision_score(y, p))\n",
        "    print(\"LogLoss:\", log_loss(y, p))\n",
        "    print(\"Brier:\", brier_score_loss(y, p))\n",
        "\n",
        "def summarize_at_threshold(y, p, thr, name=\"\"):\n",
        "    p = np.clip(p, 1e-6, 1 - 1e-6)\n",
        "    pred = (p >= thr).astype(int)\n",
        "    cm = confusion_matrix(y, pred)\n",
        "    acc = accuracy_score(y, pred)\n",
        "    bacc = balanced_accuracy_score(y, pred)\n",
        "\n",
        "    tn, fp = cm[0,0], cm[0,1]\n",
        "    fn, tp = cm[1,0], cm[1,1]\n",
        "    precision = tp / max(tp + fp, 1)\n",
        "    recall    = tp / max(tp + fn, 1)\n",
        "\n",
        "    print(f\"\\n--- {name} @ thr={thr:.4f} ---\")\n",
        "    print(\"Confusion:\\n\", cm)\n",
        "    print(\"Accuracy:\", acc)\n",
        "    print(\"Balanced accuracy:\", bacc)\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "\n",
        "def topk_report(y, p, name=\"\"):\n",
        "    p = np.asarray(p)\n",
        "    y = np.asarray(y)\n",
        "    total_pos = max(int(y.sum()), 1)\n",
        "    for k in TOP_K_LIST:\n",
        "        thr = float(np.quantile(p, 1 - k/100.0))\n",
        "        sel = p >= thr\n",
        "        capture = int(y[sel].sum()) / total_pos\n",
        "        prec = float(y[sel].mean()) if sel.sum() else 0.0\n",
        "        print(f\"Top {k:>2}%: thr={thr:.4f}  capture={capture:.3f}  precision={prec:.3f}\")\n",
        "\n",
        "# ===================== SPLITS =====================\n",
        "train_df = df[df[\"tYear\"].isin(TRAIN_YEARS)].copy()\n",
        "test_df  = df[df[\"tYear\"].eq(TEST_YEAR)].copy()\n",
        "\n",
        "Xtr = train_df[feature_cols].to_numpy(np.float32)\n",
        "ytr = train_df[\"label\"].to_numpy(np.int32)\n",
        "\n",
        "Xte = test_df[feature_cols].to_numpy(np.float32)\n",
        "yte = test_df[\"label\"].to_numpy(np.int32)\n",
        "\n",
        "Xu  = df_u[feature_cols].to_numpy(np.float32)\n",
        "yu  = df_u[\"label\"].to_numpy(np.int32)\n",
        "\n",
        "print(\"Train split:\", TRAIN_YEARS, \"n=\", len(ytr), \"pos rate=\", ytr.mean())\n",
        "print(\"Test year:\", TEST_YEAR, \"n=\", len(yte), \"pos rate=\", yte.mean())\n",
        "print(\"Unbiased year:\", int(df_u[\"tYear\"].iloc[0]) if \"tYear\" in df_u.columns else \"?\", \"n=\", len(yu), \"pos rate=\", yu.mean())\n",
        "\n",
        "# ===================== 1) LOGISTIC REGRESSION =====================\n",
        "logit = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"clf\", LogisticRegression(\n",
        "        solver=\"lbfgs\",\n",
        "        max_iter=5000,\n",
        "        C=1.0\n",
        "    ))\n",
        "])\n",
        "\n",
        "logit.fit(Xtr, ytr)\n",
        "\n",
        "# Time-split eval (balanced-ish)\n",
        "p_te = logit.predict_proba(Xte)[:, 1]\n",
        "eval_probs(yte, p_te, name=\"Logit: time-split test (balanced)\")\n",
        "summarize_at_threshold(yte, p_te, thr=0.5, name=\"Logit time-split (thr=0.5)\")\n",
        "topk_report(yte, p_te, name=\"Logit time-split\")\n",
        "\n",
        "# Unbiased eval (natural base rate)\n",
        "p_u = logit.predict_proba(Xu)[:, 1]\n",
        "eval_probs(yu, p_u, name=\"Logit: unbiased forest-only\")\n",
        "summarize_at_threshold(yu, p_u, thr=0.5, name=\"Logit unbiased (thr=0.5)\")\n",
        "topk_report(yu, p_u, name=\"Logit unbiased\")\n",
        "\n",
        "# ---- Calibrate on unbiased set (prefit model) ----\n",
        "logit_frozen = FrozenEstimator(logit)  # logit is already fit on (Xtr, ytr)\n",
        "logit_cal = CalibratedClassifierCV(logit_frozen, method=\"sigmoid\", cv=5)\n",
        "logit_cal.fit(Xu, yu)\n",
        "\n",
        "p_u_cal = logit_cal.predict_proba(Xu)[:, 1]\n",
        "eval_probs(yu, p_u_cal, name=\"Logit: unbiased (calibrated)\")\n",
        "summarize_at_threshold(yu, p_u_cal, thr=0.5, name=\"Logit calibrated unbiased (thr=0.5)\")\n",
        "topk_report(yu, p_u_cal, name=\"Logit calibrated unbiased\")\n",
        "\n",
        "# ===================== Export w,b for menagerie =====================\n",
        "# IMPORTANT: if you use StandardScaler, the effective w,b in ORIGINAL A-space\n",
        "# are different from clf.coef_. We compute the equivalent w,b in raw feature space.\n",
        "scaler = logit.named_steps[\"scaler\"]\n",
        "clf = logit.named_steps[\"clf\"]\n",
        "\n",
        "w_scaled = clf.coef_.ravel()\n",
        "b_scaled = float(clf.intercept_[0])\n",
        "\n",
        "# Convert to raw-space weights: w_raw = w_scaled / scale_\n",
        "# intercept_raw = b_scaled - sum(w_scaled * mean_/scale_)\n",
        "w_raw = w_scaled / scaler.scale_\n",
        "b_raw = b_scaled - float(np.sum(w_scaled * (scaler.mean_ / scaler.scale_)))\n",
        "\n",
        "w_raw_str = \",\".join([f\"{v:.10g}\" for v in w_raw])\n",
        "print(\"\\n=== MENAGERIE PARAMS (LOGIT, RAW SPACE) ===\")\n",
        "print(\"b:\", f\"{b_raw:.10g}\")\n",
        "print(\"w:\", w_raw_str[:160], \"... (len=\", len(w_raw_str), \")\")\n",
        "\n",
        "# ===================== 2) LINEAR SVM BASELINE (CALIBRATED) =====================\n",
        "# LinearSVC gives margins; calibrate to probabilities for comparison/map.\n",
        "svm = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"svm\", LinearSVC(C=1.0, max_iter=8000))\n",
        "])\n",
        "svm.fit(Xtr, ytr)\n",
        "\n",
        "svm_frozen = FrozenEstimator(svm)  # svm is already fit on (Xtr, ytr)\n",
        "svm_cal = CalibratedClassifierCV(svm_frozen, method=\"sigmoid\", cv=5)\n",
        "svm_cal.fit(Xu, yu)\n",
        "\n",
        "p_u_svm = svm_cal.predict_proba(Xu)[:, 1]\n",
        "eval_probs(yu, p_u_svm, name=\"SVM (calibrated): unbiased forest-only\")\n",
        "summarize_at_threshold(yu, p_u_svm, thr=0.5, name=\"SVM calibrated unbiased (thr=0.5)\")\n",
        "topk_report(yu, p_u_svm, name=\"SVM calibrated unbiased\")\n",
        "\n",
        "print(\"\\nDone.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K74OR-AsUEH9",
        "outputId": "12e0cc65-7d55-4630-cc5f-be373032e038"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dist_to_nonforest_m    2.936693\n",
            "A54                    1.890045\n",
            "A61                    1.305763\n",
            "A43                    1.203202\n",
            "A24                    1.135789\n",
            "A53                    1.006012\n",
            "A33                    0.994663\n",
            "A21                    0.990465\n",
            "A35                    0.986301\n",
            "A50                    0.974518\n",
            "A30                    0.936797\n",
            "A09                    0.836124\n",
            "A62                    0.827375\n",
            "A52                    0.791289\n",
            "A25                    0.715737\n",
            "A01                    0.702006\n",
            "A56                    0.645595\n",
            "A15                    0.583833\n",
            "A07                    0.577139\n",
            "A63                    0.577114\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# After fitting logit\n",
        "names = feature_cols\n",
        "w = logit.named_steps[\"clf\"].coef_.ravel()\n",
        "\n",
        "# Convert to comparable \"importance\": abs(weight) in standardized space\n",
        "imp = pd.Series(np.abs(w), index=names).sort_values(ascending=False)\n",
        "print(imp.head(20))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97dYWOQ3XOnf",
        "outputId": "2fc8c797-cd4a-4d93-b690-dec086468c3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Embeddings only ===\n",
            "Unbiased ROC: 0.9457982158396754\n",
            "Unbiased PR : 0.35327331651499805\n",
            "Top  1%: thr=0.9657  capture=0.174  precision=0.511\n",
            "Top  2%: thr=0.9330  capture=0.297  precision=0.438\n",
            "Top  5%: thr=0.7993  capture=0.529  precision=0.312\n",
            "Top 10%: thr=0.5136  capture=0.756  precision=0.223\n",
            "\n",
            "=== Context only ===\n",
            "Unbiased ROC: 0.8642037219219756\n",
            "Unbiased PR : 0.11724599106332922\n",
            "Top  1%: thr=0.6503  capture=0.056  precision=0.163\n",
            "Top  2%: thr=0.6500  capture=0.101  precision=0.149\n",
            "Top  5%: thr=0.6496  capture=0.222  precision=0.131\n",
            "Top 10%: thr=0.6483  capture=0.362  precision=0.107\n",
            "\n",
            "=== Embeddings + Context ===\n",
            "Unbiased ROC: 0.9449720053774658\n",
            "Unbiased PR : 0.37074507637140347\n",
            "Top  1%: thr=0.9788  capture=0.179  precision=0.525\n",
            "Top  2%: thr=0.9568  capture=0.307  precision=0.452\n",
            "Top  5%: thr=0.8767  capture=0.543  precision=0.321\n",
            "Top 10%: thr=0.6605  capture=0.758  precision=0.224\n"
          ]
        }
      ],
      "source": [
        "def fit_eval(cols, label):\n",
        "    Xtr_ = train_df[cols].to_numpy(np.float32)\n",
        "    Xte_ = test_df[cols].to_numpy(np.float32)\n",
        "\n",
        "    model = Pipeline([\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"clf\", LogisticRegression(solver=\"lbfgs\", max_iter=5000, C=1.0))\n",
        "    ])\n",
        "    model.fit(Xtr_, ytr)\n",
        "\n",
        "    # unbiased\n",
        "    Xu_ = df_u[cols].to_numpy(np.float32)\n",
        "    pu_ = model.predict_proba(Xu_)[:, 1]\n",
        "    print(\"\\n===\", label, \"===\")\n",
        "    print(\"Unbiased ROC:\", roc_auc_score(yu, pu_))\n",
        "    print(\"Unbiased PR :\", average_precision_score(yu, pu_))\n",
        "    topk_report(yu, pu_, name=label)\n",
        "\n",
        "embed_cols = [f\"A{i:02d}\" for i in range(64)]\n",
        "ctx_cols = [\"dist_to_nonforest_m\", \"dist_to_road_m\"]\n",
        "\n",
        "fit_eval(embed_cols, \"Embeddings only\")\n",
        "fit_eval(ctx_cols,   \"Context only\")\n",
        "fit_eval(embed_cols + ctx_cols, \"Embeddings + Context\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nfOGLxUXTlx",
        "outputId": "1c1b89d2-f402-422f-d486-993f200d8566"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " dist_to_nonforest_m\n",
            "mean label0: 5073.617819309574 median0: 2701.5541623443787\n",
            "mean label1: 702.0927946264575 median1: 463.31271652791656\n",
            "\n",
            " dist_to_road_m\n",
            "mean label0: 18122.892275896695 median0: 14800.239870454232\n",
            "mean label1: 15483.844292893737 median1: 8231.379779855364\n"
          ]
        }
      ],
      "source": [
        "for col in [\"dist_to_nonforest_m\", \"dist_to_road_m\"]:\n",
        "    s0 = df_u[df_u[\"label\"]==0][col].dropna()\n",
        "    s1 = df_u[df_u[\"label\"]==1][col].dropna()\n",
        "    print(\"\\n\", col)\n",
        "    print(\"mean label0:\", float(s0.mean()), \"median0:\", float(s0.median()))\n",
        "    print(\"mean label1:\", float(s1.mean()), \"median1:\", float(s1.median()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "zyyr6K_kXbnC"
      },
      "outputs": [],
      "source": [
        "#####spatial holdout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "pKkHqvIiYW4i"
      },
      "outputs": [],
      "source": [
        "\n",
        "def add_lonlat_and_tiles(df: pd.DataFrame, tile_km: float = 50.0) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Adds columns: lon, lat, tile_x, tile_y, tile_id\n",
        "    tile_km: approx tile size in km for spatial blocking (try 25, 50, 100).\n",
        "    \"\"\"\n",
        "    def parse_pt(g):\n",
        "        if pd.isna(g):\n",
        "            return np.nan, np.nan\n",
        "        obj = json.loads(g)\n",
        "        # Expect {\"type\":\"Point\",\"coordinates\":[lon,lat]}\n",
        "        lon, lat = obj[\"coordinates\"]\n",
        "        return lon, lat\n",
        "\n",
        "    coords = df[\".geo\"].apply(parse_pt)\n",
        "    df = df.copy()\n",
        "    df[\"lon\"] = coords.apply(lambda t: t[0])\n",
        "    df[\"lat\"] = coords.apply(lambda t: t[1])\n",
        "\n",
        "    # Approx degrees per km\n",
        "    deg_per_km_lat = 1.0 / 111.32\n",
        "    # lon degrees shrink by cos(lat)\n",
        "    deg_per_km_lon = deg_per_km_lat / np.cos(np.deg2rad(df[\"lat\"].clip(-89.9, 89.9)))\n",
        "\n",
        "    tile_h = tile_km * deg_per_km_lat\n",
        "    tile_w = tile_km * deg_per_km_lon\n",
        "\n",
        "    # Use global origin (0,0) grid\n",
        "    df[\"tile_x\"] = np.floor(df[\"lon\"] / tile_w).astype(\"Int64\")\n",
        "    df[\"tile_y\"] = np.floor(df[\"lat\"] / tile_h).astype(\"Int64\")\n",
        "    df[\"tile_id\"] = df[\"tile_x\"].astype(str) + \"_\" + df[\"tile_y\"].astype(str)\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "JtOcGH6uYXiT"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def eval_probs(y, p, name=\"\"):\n",
        "    p = np.clip(p, 1e-6, 1 - 1e-6)\n",
        "    print(f\"\\n=== {name} ===\")\n",
        "    print(\"n:\", len(y), \"pos rate:\", float(y.mean()))\n",
        "    print(\"ROC-AUC:\", roc_auc_score(y, p))\n",
        "    print(\"PR-AUC:\", average_precision_score(y, p))\n",
        "    print(\"LogLoss:\", log_loss(y, p))\n",
        "    print(\"Brier:\", brier_score_loss(y, p))\n",
        "\n",
        "def spatial_holdout_eval(\n",
        "    train_df: pd.DataFrame,\n",
        "    test_df: pd.DataFrame,\n",
        "    feature_cols: list[str],\n",
        "    tile_km: float = 50.0,\n",
        "    test_tile_frac: float = 0.25,\n",
        "    seed: int = 42,\n",
        "    name: str = \"\"\n",
        "):\n",
        "    # Add tiles\n",
        "    tr = add_lonlat_and_tiles(train_df, tile_km=tile_km)\n",
        "    te = add_lonlat_and_tiles(test_df, tile_km=tile_km)\n",
        "\n",
        "    # Hold out a subset of tiles from *training* years (spatial holdout)\n",
        "    rng = np.random.default_rng(seed)\n",
        "    tiles = tr[\"tile_id\"].dropna().unique()\n",
        "    rng.shuffle(tiles)\n",
        "    n_test_tiles = max(1, int(len(tiles) * test_tile_frac))\n",
        "    holdout_tiles = set(tiles[:n_test_tiles])\n",
        "\n",
        "    tr_in  = tr[~tr[\"tile_id\"].isin(holdout_tiles)]\n",
        "    tr_out = tr[ tr[\"tile_id\"].isin(holdout_tiles)]\n",
        "\n",
        "    print(\"\\n--- Spatial split setup ---\")\n",
        "    print(\"tile_km:\", tile_km, \"unique tiles:\", len(tiles), \"holdout tiles:\", len(holdout_tiles))\n",
        "    print(\"train-in n:\", len(tr_in), \"pos:\", tr_in[\"label\"].mean())\n",
        "    print(\"train-out n:\", len(tr_out), \"pos:\", tr_out[\"label\"].mean())\n",
        "\n",
        "    # Fit on in-tiles\n",
        "    X_in = tr_in[feature_cols].to_numpy(np.float32)\n",
        "    y_in = tr_in[\"label\"].to_numpy(np.int32)\n",
        "\n",
        "    model = Pipeline([\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"clf\", LogisticRegression(solver=\"lbfgs\", max_iter=5000, C=1.0))\n",
        "    ])\n",
        "    model.fit(X_in, y_in)\n",
        "\n",
        "    # Evaluate on held-out tiles (same years distribution, new space)\n",
        "    X_out = tr_out[feature_cols].to_numpy(np.float32)\n",
        "    y_out = tr_out[\"label\"].to_numpy(np.int32)\n",
        "    p_out = model.predict_proba(X_out)[:, 1]\n",
        "    eval_probs(y_out, p_out, name=f\"{name} TRAIN-years spatial holdout tiles\")\n",
        "\n",
        "    # Evaluate on unbiased forest-only (usually 2022) as well\n",
        "    X_te = te[feature_cols].to_numpy(np.float32)\n",
        "    y_te = te[\"label\"].to_numpy(np.int32)\n",
        "    p_te = model.predict_proba(X_te)[:, 1]\n",
        "    eval_probs(y_te, p_te, name=f\"{name} UNBIASED (forest-only)\")\n",
        "\n",
        "    return model, holdout_tiles\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7r79lkEYZqx",
        "outputId": "d8a161f1-641e-400d-c0e5-baf10aebb9c8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_43555/1330842671.py:26: UserWarning: you are shuffling a 'StringArray' object which is not a subclass of 'Sequence'; `shuffle` is not guaranteed to behave correctly. E.g., non-numpy array/tensor objects with view semantics may contain duplicates after shuffling.\n",
            "  rng.shuffle(tiles)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Spatial split setup ---\n",
            "tile_km: 50.0 unique tiles: 33 holdout tiles: 8\n",
            "train-in n: 20890 pos: 0.36065102920057446\n",
            "train-out n: 2206 pos: 0.2547597461468722\n",
            "\n",
            "=== LOGIT (emb+ctx) TRAIN-years spatial holdout tiles ===\n",
            "n: 2206 pos rate: 0.2547597461468722\n",
            "ROC-AUC: 0.9734254184308734\n",
            "PR-AUC: 0.9132593618513314\n",
            "LogLoss: 0.1797126790459221\n",
            "Brier: 0.056742638313342274\n",
            "\n",
            "=== LOGIT (emb+ctx) UNBIASED (forest-only) ===\n",
            "n: 14026 pos rate: 0.029516612006274062\n",
            "ROC-AUC: 0.9456202327869271\n",
            "PR-AUC: 0.37546526177553585\n",
            "LogLoss: 0.2640176756058159\n",
            "Brier: 0.08129834279635728\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Choose which features:\n",
        "embed_cols = [f\"A{i:02d}\" for i in range(64)]\n",
        "ctx_cols = [\"dist_to_nonforest_m\", \"dist_to_road_m\"]\n",
        "feature_cols = embed_cols + ctx_cols  # or embed_cols only\n",
        "\n",
        "model_spatial, tiles = spatial_holdout_eval(\n",
        "    train_df=train_df,\n",
        "    test_df=df_u,                 # unbiased forest-only\n",
        "    feature_cols=feature_cols,\n",
        "    tile_km=50.0,                 # try 25, 50, 100\n",
        "    test_tile_frac=0.25,\n",
        "    seed=42,\n",
        "    name=\"LOGIT (emb+ctx)\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Um2Uzj5sM9K"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
